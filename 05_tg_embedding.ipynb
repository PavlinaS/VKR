{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909a5c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1deae24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b77c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = pd.read_csv('tg_join_lab.csv', sep='|')\n",
    "f = pd.read_pickle('tg_join_lab.pkl')\n",
    "# print(f.shape)\n",
    "# f['obs'] = 1\n",
    "# f = f.sort_values('date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dadb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = f[f.ticker.isin(['sber', 'gazp', 'lkoh', 'vtbr'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604a26b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((125165,), (125165,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = f.text_reg_stop_lem\n",
    "target = f.ticker\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaf8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "proc_words = [text.split() for text in data]\n",
    "embeddings_trained = Word2Vec(proc_words, # data for model to train on\n",
    "                 vector_size=100,                 # embedding vector size\n",
    "                 min_count=3,             # consider words that occured at least 5 times\n",
    "                 window=5).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ace0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sum(comment, embeddings):\n",
    "    \"\"\"\n",
    "    implement a function that converts preprocessed comment to a sum of token vectors\n",
    "    \"\"\"\n",
    "    embedding_dim = embeddings.vectors.shape[1]\n",
    "    features = np.zeros([embedding_dim], dtype='float32')\n",
    "\n",
    "    for word in comment.split():\n",
    "        if word in embeddings:\n",
    "            features += embeddings[f'{word}']\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aea63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f65237",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70667573",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wv = [list(i) for i in X_wv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f00151",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['emb'] = X_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57c251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['bh'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee4946db",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_14148/3013829595.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\79627\\AppData\\Local\\Temp/ipykernel_14148/3013829595.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    label_1_0.05\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "label_1_0.05\n",
    "label_1_0.1\n",
    "label_1_0.2\n",
    "label_1_0.3\n",
    "label_1_0.5\n",
    "label_1_0.8\n",
    "label_1_1.0\n",
    "label_1_2.0\n",
    "label_1_3.0\n",
    "label_1_4.0\n",
    "label_1_5.0\n",
    "label_1_6.0\n",
    "label_1_7.0\n",
    "label_1_8.0\n",
    "label_1_9.0\n",
    "label_1_10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43797531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bane    491\n",
       "agro    478\n",
       "rlmn    458\n",
       "iskj    458\n",
       "unac    451\n",
       "kmaz    445\n",
       "nmtp    443\n",
       "flot    442\n",
       "trmk    388\n",
       "tanl    383\n",
       "life    374\n",
       "gche    369\n",
       "enru    352\n",
       "irkt    339\n",
       "svav    289\n",
       "sibn    273\n",
       "lsrg    269\n",
       "ingr    258\n",
       "usbn    243\n",
       "msng    228\n",
       "etln    219\n",
       "mstt    214\n",
       "selg    197\n",
       "tgkb    182\n",
       "kzos    157\n",
       "gltr    154\n",
       "obuv    151\n",
       "cntl    150\n",
       "arsa    128\n",
       "acko      9\n",
       "Name: ticker, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.ticker.value_counts().tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91eab71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yndx\n",
      "lsng\n",
      "aflt\n",
      "qiwi\n",
      "rasp\n",
      "gmkn\n",
      "five\n",
      "hydr\n",
      "rbcm\n",
      "sngs\n",
      "pikk\n",
      "tors\n",
      "moex\n",
      "rtkm\n",
      "tgkd\n",
      "tcsg\n",
      "bane\n",
      "belu\n",
      "iskj\n",
      "irkt\n",
      "irao\n",
      "lkoh\n",
      "mgnt\n",
      "vtbr\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "field = 'emb'\n",
    "out = pd.DataFrame()\n",
    "\n",
    "# min_dfs = [5, 10, 15, 21, 28, 36, 45, 55, 66] \n",
    "# max_dfs = [0.5]\n",
    "# ngram_range_maxs = [1]\n",
    "\n",
    "# days = [1]\n",
    "\n",
    "# min_dfs = [5] \n",
    "# max_dfs = [0.5]\n",
    "# ngram_range_maxs = [1]\n",
    "j_neuts = [0.05]\n",
    "j_poss = [8.0] \n",
    "j_negs = [8.0]\n",
    "days = [1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "counter = 0\n",
    "i = 1\n",
    "t = 4\n",
    "\n",
    "vector_sizes = [30, 50, 100, 150, 300]\n",
    "min_counts = [3, 7, 10, 20]\n",
    "windows = [3, 7, 10, 20]\n",
    "# vector_sizes = [30]\n",
    "# min_counts = [3, 7]\n",
    "# windows = [3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# split\n",
    "for j_neut in j_neuts:\n",
    "    for j_pos in j_poss:\n",
    "        for j_neg in j_negs:\n",
    "            label_neut = f'label_{i}_{j_neut}'\n",
    "            label_pos = f'label_{i}_{j_pos}'\n",
    "            label_neg = f'label_{i}_{j_neg}'\n",
    "            for day in days:\n",
    "                for vector_size_i in vector_sizes:\n",
    "                    for min_count_i in min_counts:\n",
    "                        for window_i in windows:\n",
    "                            for ticker in f.ticker.unique(): #['bane']: # \n",
    "                                print(ticker)\n",
    "                                try:\n",
    "                                    embeddings_trained = Word2Vec(proc_words, # data for model to train on\n",
    "                                                                  vector_size=vector_size_i,                 # embedding vector size\n",
    "                                                                  min_count=min_count_i,             # consider words that occured at least 5 times\n",
    "                                                                  window=window_i).wv\n",
    "\n",
    "                                    X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in data])\n",
    "\n",
    "                                    f['emb'] = [list(i) for i in X_wv]\n",
    "\n",
    "                                    ps = f[[label_neut, label_pos, label_neg, field]][(f.ticker!=ticker)] \n",
    "                                    ps = ps.loc[:,~ps.columns.duplicated()]\n",
    "                                    if t == 0:\n",
    "                                        min_df = min(ps[(ps[label_pos]==1)].shape[0], \\\n",
    "                                                     ps[(ps[label_neg]==-1)].shape[0])\n",
    "\n",
    "                                        ps1 = ps[(ps[label_pos]==1)].sample(frac=1, random_state=41)\n",
    "                                        ps2 = ps[(ps[label_neg]==-1)].sample(frac=1, random_state=42)\n",
    "\n",
    "                                        df = pd.concat([ps1[:min_df].rename(columns={label_pos:'sentiment'}), \\\n",
    "                                                        ps2[:min_df].rename(columns={label_neg:'sentiment'}), \\\n",
    "                                                       ]).sample(frac=1, random_state=44)\n",
    "\n",
    "                                    else:\n",
    "                                        min_df = min(ps[(ps[label_pos]==1)].shape[0], \\\n",
    "                                                     ps[(ps[label_neut]==0)].shape[0], \\\n",
    "                                                     ps[(ps[label_neg]==-1)].shape[0])\n",
    "\n",
    "                                        ps1 = ps[(ps[label_pos]==1)][[label_pos, field]].sample(frac=1, random_state=41)\n",
    "                                        ps2 = ps[(ps[label_neut]==0)][[label_neut, field]].sample(frac=1, random_state=42)\n",
    "                                        ps3 = ps[(ps[label_neg]==-1)][[label_neg, field]].sample(frac=1, random_state=43)\n",
    "\n",
    "                                        df = pd.concat([ps1[:min_df].rename(columns={label_pos:'sentiment'}), \\\n",
    "                                                        ps2[:min_df].rename(columns={label_neut:'sentiment'}), \\\n",
    "                                                        ps3[:min_df].rename(columns={label_neg:'sentiment'}) \\\n",
    "                                                       ]).sample(frac=1, random_state=44)\n",
    "\n",
    "\n",
    "                                    x_train = [list(i) for i in df[field]] \n",
    "                                    y_train = df['sentiment'] \n",
    "\n",
    "                                # fit\n",
    "                #                     for ngram_range_max in ngram_range_maxs:\n",
    "                #                         for min_df in min_dfs:\n",
    "                #                             for max_df in max_dfs:\n",
    "\n",
    "                #                                 tf_idf = TfidfVectorizer(min_df=min_df, \\\n",
    "                #                                                          max_df=max_df, \\\n",
    "                #                                                          ngram_range=(1, ngram_range_max))\n",
    "\n",
    "                                    train_features = x_train\n",
    "\n",
    "\n",
    "                                    model = DecisionTreeClassifier(min_samples_leaf=2, random_state=7, max_depth=6)\n",
    "                                    model.fit(train_features, y_train)\n",
    "\n",
    "\n",
    "                                    y_train_pred = model.predict(train_features)\n",
    "                # В идеале волатильность взять за эти же лаги наверное\n",
    "                                    df_temp = pd.DataFrame(f[(f.ticker==ticker)]\\\n",
    "                                                           [[field, 'obs', 'date',\\\n",
    "                                                             f'change_close_{day}', 'bh', 'close']]).\\\n",
    "                reset_index().drop('index', axis=1)\n",
    "\n",
    "                                    y_pred = model.predict([list(i) for i in df_temp[field]])\n",
    "\n",
    "\n",
    "\n",
    "                                    df_temp['sentiment'] = y_pred\n",
    "\n",
    "                                    df_temp['neg'] = 0\n",
    "                                    df_temp['neg'][df_temp.sentiment==-1] = 1\n",
    "\n",
    "                                    df_temp['pos'] = 0\n",
    "                                    df_temp['pos'][df_temp.sentiment==1] = 1\n",
    "\n",
    "                                    df_temp['neut'] = 0\n",
    "                                    df_temp['neut'][df_temp.sentiment==0] = 1\n",
    "                                    df_temp.emb = df_temp.emb.astype(str)\n",
    "                                    df_temp = df_temp.drop_duplicates().\\\n",
    "                                    groupby(['date', f'change_close_{day}', 'bh', 'close']).sum().reset_index()\n",
    "\n",
    "                # Делаем сум роллинг окно для суммирования сентиментов для периодов больше 1 дня \n",
    "                # Нужно удостовериться в сортировке должно быть по возрастанию даты\n",
    "\n",
    "                                    df_temp[f'close_lag_1'] = df_temp.close.shift(periods=1)\n",
    "                                    #df_temp[f'close_change_lag_1'] = df_temp.close.shift(periods=1)\n",
    "                                    df_temp = df_temp.sort_values('date', ascending=True)\n",
    "\n",
    "                                    df_temp['neg'] = df_temp['neg'].rolling(day).sum()\n",
    "                                    df_temp['pos'] = df_temp['pos'].rolling(day).sum()\n",
    "                                    df_temp['neut'] = df_temp['neut'].rolling(day).sum()\n",
    "\n",
    "                                    df_temp.dropna(subset=['neg', 'pos', 'neut', 'close_lag_1'], inplace=True)\n",
    "                    # просто сумма\n",
    "                                    df_temp['sum_sentiment'] = 0\n",
    "                                    df_temp['sum_sentiment'][(df_temp.pos>df_temp.neg)&(df_temp.pos>=df_temp.neut)] = 1\n",
    "                                    df_temp['sum_sentiment'][(df_temp.pos<df_temp.neg)&(df_temp.neg>=df_temp.neut)] = -1\n",
    "\n",
    "                # дивергенция c добавлением \n",
    "                                    df_temp['diver'] = ((df_temp.pos - df_temp.neg)**2 \\\n",
    "                                                              / (df_temp.pos + df_temp.neg)**2)**(1/2)\n",
    "\n",
    "                                    #df_temp.reset_index(level=[f'change_close_{day}', 'date', 'bh'], inplace=True)\n",
    "                                    df_temp['clear_return'] = df_temp.sum_sentiment * df_temp[f'change_close_{day}']\n",
    "                                    df_temp['diver_return'] = df_temp.clear_return * df_temp.diver                \n",
    "\n",
    "                                    res = dict()\n",
    "                                    res['counter'] = [counter]\n",
    "                                    res['ticker'] = [ticker]\n",
    "                                    res['date'] = [day]\n",
    "\n",
    "                                    res['min_df'] = [min_df]\n",
    "\n",
    "                                    res['j_neut'] = [j_neut]\n",
    "                                    res['j_pos'] = [j_pos] \n",
    "                                    res['j_neg'] = [j_neg] \n",
    "\n",
    "                                    res['train_size'] = [y_train.shape[0]]\n",
    "                                    res['test_size'] = [y_pred.shape[0]]\n",
    "\n",
    "                                    res['obs'] = \\\n",
    "                                    [(df_temp.obs[df_temp.clear_return.isna()==False]).sum()]\n",
    "                                    res['bh'] = df_temp.bh.max()\n",
    "                                    res['original_return_isna_all'] = \\\n",
    "                                    [(df_temp[f'change_close_{day}']\\\n",
    "                                      [df_temp[f'change_close_{day}'].isna()==False]).sum()]\n",
    "\n",
    "\n",
    "                                    res['model_return_isna_all'] = \\\n",
    "                                    [(df_temp.clear_return[df_temp.clear_return.isna()==False]).sum()]\n",
    "\n",
    "                                    res['model_return_isna_short'] = \\\n",
    "                                    [(df_temp.clear_return[(df_temp.clear_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=1)]).sum()]\n",
    "\n",
    "                                    res['model_return_isna_long'] = \\\n",
    "                                    [(df_temp.clear_return[(df_temp.clear_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=-1)]).sum()]\n",
    "\n",
    "\n",
    "                                    res['model_diver_return_isna_all'] = \\\n",
    "                                    [(df_temp.diver_return[df_temp.diver_return.isna()==False]).sum()]\n",
    "\n",
    "                                    res['model_diver_return_isna_short'] = \\\n",
    "                                    [(df_temp.diver_return[(df_temp.diver_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=1)]).sum()]\n",
    "\n",
    "                                    res['model_diver_return_isna_long'] = \\\n",
    "                                    [(df_temp.diver_return[(df_temp.diver_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=-1)]).sum()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                    res['train_accuracy'] = [metrics.accuracy_score(y_train_pred, y_train)]\n",
    "\n",
    "\n",
    "                                    res['train_precision_macro'] = [metrics.precision_score(y_train_pred, y_train\\\n",
    "                                                                                            , average='macro')]\n",
    "\n",
    "\n",
    "                                    res['train_precision_micro'] = [metrics.precision_score(y_train_pred, y_train\\\n",
    "                                                                                            , average='micro')]\n",
    "\n",
    "\n",
    "                                    res['train_recall_micro'] = [metrics.recall_score(y_train_pred, y_train\\\n",
    "                                                                                      , average='micro')]\n",
    "\n",
    "\n",
    "                                    res['train_recall_macro'] = [metrics.recall_score(y_train_pred, y_train\\\n",
    "                                                                                      , average='macro')]\n",
    "\n",
    "                                    res['train_f1_micro'] = [metrics.f1_score(y_train_pred, y_train\\\n",
    "                                                                              , average='micro')]\n",
    "\n",
    "\n",
    "                                    res['train_f1_macro'] = [metrics.f1_score(y_train_pred, y_train\\\n",
    "                                                                              , average='macro')]\n",
    "\n",
    "\n",
    "\n",
    "                                    x = df_temp[['sum_sentiment', 'close_lag_1']]\n",
    "                                    x.sum_sentiment = x.sum_sentiment.astype(str)\n",
    "\n",
    "                                    x = pd.get_dummies(x, drop_first=True)\n",
    "                                    y = df_temp.close\n",
    "                                    x2 = sm.add_constant(x)\n",
    "                                    lm = sm.OLS(y, x2)\n",
    "                                    res_lm = lm.fit()\n",
    "\n",
    "                                    res['const'] = res_lm.params[0]\n",
    "                                    res[f'{x.columns[0]}_coef'] = res_lm.params[1]\n",
    "                                    res[f'{x.columns[1]}_coef'] = res_lm.params[2]\n",
    "                                    res[f'{x.columns[2]}_coef'] = res_lm.params[3]\n",
    "\n",
    "                                    res['const_p'] = res_lm.pvalues[0]\n",
    "                                    res[f'{x.columns[0]}_p'] = res_lm.pvalues[1]\n",
    "                                    res[f'{x.columns[1]}_p'] = res_lm.pvalues[2]\n",
    "                                    res[f'{x.columns[2]}_p'] = res_lm.pvalues[3]\n",
    "                                    res['r2'] = res_lm.rsquared\n",
    "                                    res['nobs'] = res_lm.nobs\n",
    "                                    \n",
    "                                    res['vector_size'] = vector_size_i\n",
    "                                    res['min_count'] = min_count_i\n",
    "                                    res['window'] = window_i\n",
    "                                    counter += 1\n",
    "                                    print(counter, end='\\r')\n",
    "\n",
    "                                    \n",
    "                                    res = pd.DataFrame.from_dict(res)\n",
    "\n",
    "                                    out = out.append(pd.DataFrame(res))\n",
    "                                    if counter % 100 == 0: \n",
    "            #                             out.to_excel(f'dt_model_ind_final.xlsx', index=False)\n",
    "                                            out.to_excel(f'dt_model_ind_final_emb.xlsx', index=False)\n",
    "\n",
    "                                except:\n",
    "                                    continue\n",
    "    \n",
    "\n",
    "                    \n",
    "out.to_excel(f'dt_model_ind_final_emb.xlsx', index=False)\n",
    "\n",
    "# out.to_excel(f'dt_model_ind_final.xlsx', index=False)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1ad2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.to_excel(f'dt_model_ind_final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935d9c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['counter', 'ticker', 'date', 'min_df', 'j_neut', 'j_pos', 'j_neg',\n",
       "       'train_size', 'test_size', 'obs', 'bh', 'original_return_isna_all',\n",
       "       'model_return_isna_all', 'model_return_isna_short',\n",
       "       'model_return_isna_long', 'model_diver_return_isna_all',\n",
       "       'model_diver_return_isna_short', 'model_diver_return_isna_long',\n",
       "       'train_accuracy', 'train_precision_macro', 'train_precision_micro',\n",
       "       'train_recall_micro', 'train_recall_macro', 'train_f1_micro',\n",
       "       'train_f1_macro', 'const', 'close_lag_1_coef', 'sum_sentiment_0_coef',\n",
       "       'sum_sentiment_1_coef', 'const_p', 'close_lag_1_p', 'sum_sentiment_0_p',\n",
       "       'sum_sentiment_1_p', 'r2', 'nobs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "583fd8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.ticker.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deb62568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.ticker[out.sum_sentiment_0_p<=0.05].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74c51365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.ticker[out.original_return_isna_all<=out.model_return_isna_all].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19aba7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out.ticker[out.train_f1_macro>=0.55].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dcd0a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker\n",
       "mtss    0.472128\n",
       "five    0.473109\n",
       "msng    0.473399\n",
       "mvid    0.474249\n",
       "yndx    0.474389\n",
       "          ...   \n",
       "gche    0.480945\n",
       "vtbr    0.482246\n",
       "gazp    0.483338\n",
       "rual    0.483514\n",
       "tcsg    0.486835\n",
       "Name: train_f1_macro, Length: 76, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.groupby('ticker')['train_f1_macro'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3b32d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "j_neut\n",
       "0.05    0.482275\n",
       "0.10    0.481878\n",
       "0.20    0.473106\n",
       "0.50    0.473464\n",
       "Name: train_f1_macro, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.groupby('j_neut')['train_f1_macro'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87e72f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "j_pos\n",
       "2.0    0.592040\n",
       "3.0    0.587449\n",
       "4.0    0.584196\n",
       "6.0    0.573573\n",
       "8.0    0.582956\n",
       "Name: train_recall_macro, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.groupby('j_pos')['train_recall_macro'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58c1de78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "j_neg\n",
       "2.0    0.440971\n",
       "3.0    0.451597\n",
       "4.0    0.464516\n",
       "6.0    0.494132\n",
       "8.0    0.537229\n",
       "Name: train_f1_macro, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.groupby('j_neg')['train_f1_macro'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f6407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9d808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neut:  (15389,)\n",
      "neg:  (14162,)\n",
      "pos:  (16278,)\n"
     ]
    }
   ],
   "source": [
    "j_neut = 0.2\n",
    "j_pos = 2.0\n",
    "j_neg = 2.0\n",
    "i = 1\n",
    "label_neut = f'label_{i}_{j_neut}'\n",
    "label_pos = f'label_{i}_{j_pos}'\n",
    "label_neg = f'label_{i}_{j_neg}'\n",
    "print('neut: ', f[label_neut][f[label_neut]==0].shape)\n",
    "print('neg: ', f[label_neg][f[label_neg]==-1].shape)\n",
    "print('pos: ', f[label_pos][f[label_pos]==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd19156",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['bh'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a835e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sber\n",
      "vtbr\n",
      "4\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>min_df</th>\n",
       "      <th>j_neut</th>\n",
       "      <th>j_pos</th>\n",
       "      <th>j_neg</th>\n",
       "      <th>train_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>obs</th>\n",
       "      <th>...</th>\n",
       "      <th>const</th>\n",
       "      <th>close_lag_1_coef</th>\n",
       "      <th>sum_sentiment_0_coef</th>\n",
       "      <th>sum_sentiment_1_coef</th>\n",
       "      <th>const_p</th>\n",
       "      <th>close_lag_1_p</th>\n",
       "      <th>sum_sentiment_0_p</th>\n",
       "      <th>sum_sentiment_1_p</th>\n",
       "      <th>r2</th>\n",
       "      <th>nobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sber</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36609</td>\n",
       "      <td>18362</td>\n",
       "      <td>18312</td>\n",
       "      <td>...</td>\n",
       "      <td>1.030217</td>\n",
       "      <td>0.994081</td>\n",
       "      <td>0.646175</td>\n",
       "      <td>0.743559</td>\n",
       "      <td>0.037722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.993880</td>\n",
       "      <td>1542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sber</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36609</td>\n",
       "      <td>18362</td>\n",
       "      <td>18312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867078</td>\n",
       "      <td>0.994126</td>\n",
       "      <td>0.791563</td>\n",
       "      <td>1.038908</td>\n",
       "      <td>0.081581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.993905</td>\n",
       "      <td>1542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>vtbr</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39546</td>\n",
       "      <td>12557</td>\n",
       "      <td>12528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.989995</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349229</td>\n",
       "      <td>0.196240</td>\n",
       "      <td>0.990690</td>\n",
       "      <td>1388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>vtbr</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39546</td>\n",
       "      <td>12557</td>\n",
       "      <td>12528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.989598</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175544</td>\n",
       "      <td>0.857650</td>\n",
       "      <td>0.990701</td>\n",
       "      <td>1388.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   counter ticker  date  min_df  j_neut  j_pos  j_neg  train_size  test_size  \\\n",
       "0        0   sber     1       5     0.2    2.0    2.0       36609      18362   \n",
       "0        1   sber     1       5     0.5    2.0    2.0       36609      18362   \n",
       "0        2   vtbr     1       5     0.2    2.0    2.0       39546      12557   \n",
       "0        3   vtbr     1       5     0.5    2.0    2.0       39546      12557   \n",
       "\n",
       "     obs  ...     const  close_lag_1_coef  sum_sentiment_0_coef  \\\n",
       "0  18312  ...  1.030217          0.994081              0.646175   \n",
       "0  18312  ...  0.867078          0.994126              0.791563   \n",
       "0  12528  ...  0.000460          0.989995             -0.000049   \n",
       "0  12528  ...  0.000409          0.989598              0.000070   \n",
       "\n",
       "   sum_sentiment_1_coef   const_p  close_lag_1_p  sum_sentiment_0_p  \\\n",
       "0              0.743559  0.037722            0.0           0.006479   \n",
       "0              1.038908  0.081581            0.0           0.000817   \n",
       "0             -0.000068  0.000129            0.0           0.349229   \n",
       "0             -0.000009  0.000600            0.0           0.175544   \n",
       "\n",
       "   sum_sentiment_1_p        r2    nobs  \n",
       "0           0.005079  0.993880  1542.0  \n",
       "0           0.000185  0.993905  1542.0  \n",
       "0           0.196240  0.990690  1388.0  \n",
       "0           0.857650  0.990701  1388.0  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "field = 'text_reg_stop_lem'\n",
    "out = pd.DataFrame()\n",
    "\n",
    "# min_dfs = [5, 10, 15, 21, 28, 36, 45, 55, 66] \n",
    "# max_dfs = [0.5]\n",
    "# ngram_range_maxs = [1]\n",
    "# j_neuts = [0.2, 0.5]\n",
    "# j_poss = [2.0, 3.0, 4.0, 6.0] \n",
    "# j_negs = [2.0, 3.0, 4.0, 6.0]\n",
    "# days = [1]\n",
    "\n",
    "min_dfs = [5] \n",
    "max_dfs = [0.5]\n",
    "ngram_range_maxs = [1]\n",
    "j_neuts = [0.2, 0.5]\n",
    "j_poss = [2.0] \n",
    "j_negs = [2.0]\n",
    "days = [1]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "i = 1\n",
    "t = 4\n",
    "\n",
    "# split\n",
    "for ticker in  ['sber', 'vtbr']: #f.ticker.unique(): #\n",
    "    print(ticker)\n",
    "\n",
    "    for j_neut in j_neuts:\n",
    "        for j_pos in j_poss:\n",
    "            for j_neg in j_negs:\n",
    "                label_neut = f'label_{i}_{j_neut}'\n",
    "                label_pos = f'label_{i}_{j_pos}'\n",
    "                label_neg = f'label_{i}_{j_neg}'\n",
    "                for day in days:\n",
    "#                     try:\n",
    "\n",
    "                    ps = f[[label_neut, label_pos, label_neg, field]][(f.ticker!=ticker)] \n",
    "                    ps = ps.loc[:,~ps.columns.duplicated()]\n",
    "                    if t == 0:\n",
    "                        min_df = min(ps[(ps[label_pos]==1)].shape[0], \\\n",
    "                                     ps[(ps[label_neg]==-1)].shape[0])\n",
    "\n",
    "                        ps1 = ps[(ps[label_pos]==1)].sample(frac=1, random_state=41)\n",
    "                        ps2 = ps[(ps[label_neg]==-1)].sample(frac=1, random_state=42)\n",
    "\n",
    "                        df = pd.concat([ps1[:min_df].rename(columns={label_pos:'sentiment'}), \\\n",
    "                                        ps2[:min_df].rename(columns={label_neg:'sentiment'}), \\\n",
    "                                       ]).sample(frac=1, random_state=44)\n",
    "\n",
    "                    else:\n",
    "                        min_df = min(ps[(ps[label_pos]==1)].shape[0], \\\n",
    "                                     ps[(ps[label_neut]==0)].shape[0], \\\n",
    "                                     ps[(ps[label_neg]==-1)].shape[0])\n",
    "\n",
    "                        ps1 = ps[(ps[label_pos]==1)][[label_pos, field]].sample(frac=1, random_state=41)\n",
    "                        ps2 = ps[(ps[label_neut]==0)][[label_neut, field]].sample(frac=1, random_state=42)\n",
    "                        ps3 = ps[(ps[label_neg]==-1)][[label_neg, field]].sample(frac=1, random_state=43)\n",
    "\n",
    "                        df = pd.concat([ps1[:min_df].rename(columns={label_pos:'sentiment'}), \\\n",
    "                                        ps2[:min_df].rename(columns={label_neut:'sentiment'}), \\\n",
    "                                        ps3[:min_df].rename(columns={label_neg:'sentiment'}) \\\n",
    "                                       ]).sample(frac=1, random_state=44)\n",
    "\n",
    "\n",
    "                    x_train = df[field]\n",
    "                    y_train = df['sentiment'] \n",
    "\n",
    "                # fit\n",
    "                    for ngram_range_max in ngram_range_maxs:\n",
    "                        for min_df in min_dfs:\n",
    "                            for max_df in max_dfs:\n",
    "\n",
    "                                tf_idf = TfidfVectorizer(min_df=min_df, \\\n",
    "                                                         max_df=max_df, \\\n",
    "                                                         ngram_range=(1, ngram_range_max))\n",
    "\n",
    "                                train_features = tf_idf.fit_transform(x_train)\n",
    "\n",
    "\n",
    "                                model = MultinomialNB(alpha=0.1)\n",
    "                                model.fit(train_features, y_train)\n",
    "                                y_train_pred = model.predict(train_features)\n",
    "# В идеале волатильность взять за эти же лаги наверное\n",
    "                                df_temp = pd.DataFrame(f[(f.ticker==ticker)]\\\n",
    "                                                       [[field, 'obs', 'date',\\\n",
    "                                                         f'change_close_{day}', 'bh', 'close']]).\\\n",
    "    reset_index().drop('index', axis=1)\n",
    "\n",
    "                                y_pred = model.predict(tf_idf.transform(df_temp[field]))\n",
    "\n",
    "                                df_temp['sentiment'] = y_pred\n",
    "\n",
    "                                df_temp['neg'] = 0\n",
    "                                df_temp['neg'][df_temp.sentiment==-1] = 1\n",
    "\n",
    "                                df_temp['pos'] = 0\n",
    "                                df_temp['pos'][df_temp.sentiment==1] = 1\n",
    "\n",
    "                                df_temp['neut'] = 0\n",
    "                                df_temp['neut'][df_temp.sentiment==0] = 1\n",
    "\n",
    "                                df_temp = df_temp.drop_duplicates().\\\n",
    "                                groupby(['date', f'change_close_{day}', 'bh', 'close']).sum().reset_index()\n",
    "\n",
    "# Делаем сум роллинг окно для суммирования сентиментов для периодов больше 1 дня \n",
    "# Нужно удостовериться в сортировке должно быть по возрастанию даты\n",
    "\n",
    "                                df_temp[f'close_lag_1'] = df_temp.close.shift(periods=1)\n",
    "                                #df_temp[f'close_change_lag_1'] = df_temp.close.shift(periods=1)\n",
    "                                df_temp = df_temp.sort_values('date', ascending=True)\n",
    "\n",
    "                                df_temp['neg'] = df_temp['neg'].rolling(day).sum()\n",
    "                                df_temp['pos'] = df_temp['pos'].rolling(day).sum()\n",
    "                                df_temp['neut'] = df_temp['neut'].rolling(day).sum()\n",
    "\n",
    "                                df_temp.dropna(subset=['neg', 'pos', 'neut', 'close_lag_1'], inplace=True)\n",
    "                # просто сумма\n",
    "                                df_temp['sum_sentiment'] = 0\n",
    "                                df_temp['sum_sentiment'][(df_temp.pos>df_temp.neg)&(df_temp.pos>=df_temp.neut)] = 1\n",
    "                                df_temp['sum_sentiment'][(df_temp.pos<df_temp.neg)&(df_temp.neg>=df_temp.neut)] = -1\n",
    "\n",
    "# дивергенция c добавлением \n",
    "                                df_temp['diver'] = ((df_temp.pos - df_temp.neg)**2 \\\n",
    "                                                          / (df_temp.pos + df_temp.neg)**2)**(1/2)\n",
    "\n",
    "                                #df_temp.reset_index(level=[f'change_close_{day}', 'date', 'bh'], inplace=True)\n",
    "                                df_temp['clear_return'] = df_temp.sum_sentiment * df_temp[f'change_close_{day}']\n",
    "                                df_temp['diver_return'] = df_temp.clear_return * df_temp.diver                \n",
    "\n",
    "                                res = dict()\n",
    "                                res['counter'] = [counter]\n",
    "                                res['ticker'] = [ticker]\n",
    "                                res['date'] = [day]\n",
    "\n",
    "                                res['min_df'] = [min_df]\n",
    "\n",
    "                                res['j_neut'] = [j_neut]\n",
    "                                res['j_pos'] = [j_pos] \n",
    "                                res['j_neg'] = [j_neg] \n",
    "\n",
    "                                res['train_size'] = [y_train.shape[0]]\n",
    "                                res['test_size'] = [y_pred.shape[0]]\n",
    "\n",
    "                                res['obs'] = \\\n",
    "                                [(df_temp.obs[df_temp.clear_return.isna()==False]).sum()]\n",
    "                                res['bh'] = df_temp.bh.max()\n",
    "                                res['original_return_isna_all'] = \\\n",
    "                                [(df_temp[f'change_close_{day}']\\\n",
    "                                  [df_temp[f'change_close_{day}'].isna()==False]).sum()]\n",
    "\n",
    "\n",
    "                                res['model_return_isna_all'] = \\\n",
    "                                [(df_temp.clear_return[df_temp.clear_return.isna()==False]).sum()]\n",
    "\n",
    "                                res['model_return_isna_short'] = \\\n",
    "                                [(df_temp.clear_return[(df_temp.clear_return.isna()==False) & \\\n",
    "                                                             (df_temp.sum_sentiment!=1)]).sum()]\n",
    "\n",
    "                                res['model_return_isna_long'] = \\\n",
    "                                [(df_temp.clear_return[(df_temp.clear_return.isna()==False) & \\\n",
    "                                                             (df_temp.sum_sentiment!=-1)]).sum()]\n",
    "\n",
    "\n",
    "                                res['model_diver_return_isna_all'] = \\\n",
    "                                [(df_temp.diver_return[df_temp.diver_return.isna()==False]).sum()]\n",
    "\n",
    "                                res['model_diver_return_isna_short'] = \\\n",
    "                                [(df_temp.diver_return[(df_temp.diver_return.isna()==False) & \\\n",
    "                                                             (df_temp.sum_sentiment!=1)]).sum()]\n",
    "\n",
    "                                res['model_diver_return_isna_long'] = \\\n",
    "                                [(df_temp.diver_return[(df_temp.diver_return.isna()==False) & \\\n",
    "                                                             (df_temp.sum_sentiment!=-1)]).sum()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                res['train_accuracy'] = [metrics.accuracy_score(y_train_pred, y_train)]\n",
    "\n",
    "\n",
    "                                res['train_precision_macro'] = [metrics.precision_score(y_train_pred, y_train\\\n",
    "                                                                                        , average='macro')]\n",
    "\n",
    "\n",
    "                                res['train_precision_micro'] = [metrics.precision_score(y_train_pred, y_train\\\n",
    "                                                                                        , average='micro')]\n",
    "\n",
    "\n",
    "                                res['train_recall_micro'] = [metrics.recall_score(y_train_pred, y_train\\\n",
    "                                                                                  , average='micro')]\n",
    "\n",
    "\n",
    "                                res['train_recall_macro'] = [metrics.recall_score(y_train_pred, y_train\\\n",
    "                                                                                  , average='macro')]\n",
    "\n",
    "                                res['train_f1_micro'] = [metrics.f1_score(y_train_pred, y_train\\\n",
    "                                                                          , average='micro')]\n",
    "\n",
    "\n",
    "                                res['train_f1_macro'] = [metrics.f1_score(y_train_pred, y_train\\\n",
    "                                                                          , average='macro')]\n",
    "\n",
    "\n",
    "\n",
    "                                x = df_temp[['sum_sentiment', 'close_lag_1']]\n",
    "                                x.sum_sentiment = x.sum_sentiment.astype(str)\n",
    "\n",
    "                                x = pd.get_dummies(x, drop_first=True)\n",
    "                                y = df_temp.close\n",
    "                                x2 = sm.add_constant(x)\n",
    "                                lm = sm.OLS(y, x2)\n",
    "                                res_lm = lm.fit()\n",
    "\n",
    "                                res['const'] = res_lm.params[0]\n",
    "                                res[f'{x.columns[0]}_coef'] = res_lm.params[1]\n",
    "                                res[f'{x.columns[1]}_coef'] = res_lm.params[2]\n",
    "                                res[f'{x.columns[2]}_coef'] = res_lm.params[3]\n",
    "\n",
    "                                res['const_p'] = res_lm.pvalues[0]\n",
    "                                res[f'{x.columns[0]}_p'] = res_lm.pvalues[1]\n",
    "                                res[f'{x.columns[1]}_p'] = res_lm.pvalues[2]\n",
    "                                res[f'{x.columns[2]}_p'] = res_lm.pvalues[3]\n",
    "                                res['r2'] = res_lm.rsquared\n",
    "                                res['nobs'] = res_lm.nobs\n",
    "                                counter += 1\n",
    "                                print(counter, end='\\r')\n",
    "\n",
    "\n",
    "                                res = pd.DataFrame.from_dict(res)\n",
    "\n",
    "                                out = out.append(pd.DataFrame(res))\n",
    "#                                     if counter % 100 == 0: \n",
    "#                                         out.to_excel(f'/is/res/nb/nb_model_ind_final.xlsx', index=False)\n",
    "                                    \n",
    "#                     except:\n",
    "#                         continue\n",
    "    \n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "out.to_excel(f'nb_model_ind_final.xlsx', index=False)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6996db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ’®¬ ў гбва®©бвўҐ C Ё¬ҐҐв ¬ҐвЄг Windows-SSD\n",
      " ‘ҐаЁ©­л© ­®¬Ґа в®¬ : 6252-A73E\n",
      "\n",
      " ‘®¤Ґа¦Ё¬®Ґ Ї ЇЄЁ C:\\Users\\79627\\Desktop\\Љђ\\¤ ­­лҐ\n",
      "\n",
      "13.03.2022  14:50    <DIR>          .\n",
      "13.03.2022  14:50    <DIR>          ..\n",
      "12.03.2022  22:42    <DIR>          .ipynb_checkpoints\n",
      "05.03.2022  19:33           116я258 03_text_processing.ipynb\n",
      "13.03.2022  14:50            46я858 05_tg_NB.ipynb\n",
      "05.12.2021  22:14           473я071 data.xlsx\n",
      "21.02.2022  21:40        17я140я695 eikon_all.xlsb\n",
      "22.02.2022  11:14       465я152я115 read_tg_msg.csv\n",
      "06.02.2022  22:01            36я402 telegram.xlsx\n",
      "06.02.2022  19:50        99я247я225 telegram_all.xlsx\n",
      "05.03.2022  13:05       145я135я260 text_lower_ticker_flag_dt.csv\n",
      "05.03.2022  13:10       145я135я255 text_lower_tic_all_flag_all_dt_e.csv\n",
      "05.03.2022  13:10       206я290я098 text_lower_tic_all_flag_all_dt_e_reg.csv\n",
      "12.03.2022  22:47       425я701я302 tg_join_lab.csv\n",
      "03.02.2022  15:37       467я303я098 tg_msg.csv\n",
      "23.02.2022  13:34           148я602 tg_msg.ipynb\n",
      "12.03.2022  23:02       353я868я400 tg_price_sentiment_join.csv\n",
      "              14 д ©«®ў  2я325я794я639 Ў ©в\n",
      "               3 Ї Ї®Є   7я997я935я616 Ў ©в бў®Ў®¤­®\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b21b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ba6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba781d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "field = 'text_reg_spell_stop_lem'\n",
    "out = pd.DataFrame()\n",
    "\n",
    "min_dfs = [5, 7, 10, 15, 21, 28, 36, 45, 55, 66] \n",
    "max_dfs = [0.5]\n",
    "ngram_range_maxs = [1]\n",
    "j_neuts = [0.1, 0.2, 0.5, 0.05]\n",
    "j_poss = [2.0, 3.0, 5.0, 9.0] \n",
    "j_negs = [2.0, 3.0, 5.0, 9.0]\n",
    "days = [1]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "i = 1\n",
    "t = 4\n",
    "\n",
    "# split\n",
    "for ticker in  f.ticker.unique(): #['sber']: #\n",
    "    print(ticker)\n",
    "\n",
    "    for j_neut in j_neuts:\n",
    "        for j_pos in j_poss:\n",
    "            for j_neg in j_negs:\n",
    "                label_neut = f'label_{i}_{j_neut}'\n",
    "                label_pos = f'label_{i}_{j_pos}'\n",
    "                label_neg = f'label_{i}_{j_neg}'\n",
    "                for day in days:\n",
    "                    try:\n",
    "\n",
    "                        ps = f[[label_neut, label_pos, label_neg, field]][(f.ticker!=ticker)] \n",
    "                        ps = ps.loc[:,~ps.columns.duplicated()]\n",
    "                        if t == 0:\n",
    "                            min_df = min(ps[(ps[label_pos]==1)].shape[0], \\\n",
    "                                         ps[(ps[label_neg]==-1)].shape[0])\n",
    "\n",
    "                            ps1 = ps[(ps[label_pos]==1)].sample(frac=1, random_state=41)\n",
    "                            ps2 = ps[(ps[label_neg]==-1)].sample(frac=1, random_state=42)\n",
    "\n",
    "                            df = pd.concat([ps1[:min_df].rename(columns={label_pos:'sentiment'}), \\\n",
    "                                            ps2[:min_df].rename(columns={label_neg:'sentiment'}), \\\n",
    "                                           ]).sample(frac=1, random_state=44)\n",
    "\n",
    "                        else:\n",
    "                            min_df = min(ps[(ps[label_pos]==1)].shape[0], \\\n",
    "                                         ps[(ps[label_neut]==0)].shape[0], \\\n",
    "                                         ps[(ps[label_neg]==-1)].shape[0])\n",
    "\n",
    "                            ps1 = ps[(ps[label_pos]==1)][[label_pos, field]].sample(frac=1, random_state=41)\n",
    "                            ps2 = ps[(ps[label_neut]==0)][[label_neut, field]].sample(frac=1, random_state=42)\n",
    "                            ps3 = ps[(ps[label_neg]==-1)][[label_neg, field]].sample(frac=1, random_state=43)\n",
    "\n",
    "                            df = pd.concat([ps1[:min_df].rename(columns={label_pos:'sentiment'}), \\\n",
    "                                            ps2[:min_df].rename(columns={label_neut:'sentiment'}), \\\n",
    "                                            ps3[:min_df].rename(columns={label_neg:'sentiment'}) \\\n",
    "                                           ]).sample(frac=1, random_state=44)\n",
    "\n",
    "\n",
    "                        x_train = df[field]\n",
    "                        y_train = df['sentiment'] \n",
    "\n",
    "                    # fit\n",
    "                        for ngram_range_max in ngram_range_maxs:\n",
    "                            for min_df in min_dfs:\n",
    "                                for max_df in max_dfs:\n",
    "\n",
    "                                    tf_idf = TfidfVectorizer(min_df=min_df, \\\n",
    "                                                             max_df=max_df, \\\n",
    "                                                             ngram_range=(1, ngram_range_max))\n",
    "\n",
    "                                    train_features = tf_idf.fit_transform(x_train)\n",
    "\n",
    "\n",
    "                                    model = MultinomialNB(alpha=0.1)\n",
    "                                    model.fit(train_features, y_train)\n",
    "                                    y_train_pred = model.predict(train_features)\n",
    "    # В идеале волатильность взять за эти же лаги наверное\n",
    "                                    df_temp = pd.DataFrame(f[(f.ticker==ticker)]\\\n",
    "                                                           [['text_reg_spell_stop_lem', 'obs', 'date',\\\n",
    "                                                             f'change_close_{day}', 'bh', 'close']]).\\\n",
    "        reset_index().drop('index', axis=1)\n",
    "\n",
    "                                    y_pred = model.predict(tf_idf.transform(df_temp.text_reg_spell_stop_lem))\n",
    "\n",
    "                                    df_temp['sentiment'] = y_pred\n",
    "\n",
    "                                    df_temp['neg'] = 0\n",
    "                                    df_temp['neg'][df_temp.sentiment==-1] = 1\n",
    "\n",
    "                                    df_temp['pos'] = 0\n",
    "                                    df_temp['pos'][df_temp.sentiment==1] = 1\n",
    "\n",
    "                                    df_temp['neut'] = 0\n",
    "                                    df_temp['neut'][df_temp.sentiment==0] = 1\n",
    "\n",
    "                                    df_temp = df_temp.drop_duplicates().\\\n",
    "                                    groupby(['date', f'change_close_{day}', 'bh', 'close']).sum().reset_index()\n",
    "\n",
    "    # Делаем сум роллинг окно для суммирования сентиментов для периодов больше 1 дня \n",
    "    # Нужно удостовериться в сортировке должно быть по возрастанию даты\n",
    "\n",
    "                                    df_temp[f'close_lag_1'] = df_temp.close.shift(periods=1)\n",
    "                                    #df_temp[f'close_change_lag_1'] = df_temp.close.shift(periods=1)\n",
    "                                    df_temp = df_temp.sort_values('date', ascending=True)\n",
    "\n",
    "                                    df_temp['neg'] = df_temp['neg'].rolling(day).sum()\n",
    "                                    df_temp['pos'] = df_temp['pos'].rolling(day).sum()\n",
    "                                    df_temp['neut'] = df_temp['neut'].rolling(day).sum()\n",
    "\n",
    "                                    df_temp.dropna(subset=['neg', 'pos', 'neut', 'close_lag_1'], inplace=True)\n",
    "                    # просто сумма\n",
    "                                    df_temp['sum_sentiment'] = 0\n",
    "                                    df_temp['sum_sentiment'][(df_temp.pos>df_temp.neg)&(df_temp.pos>=df_temp.neut)] = 1\n",
    "                                    df_temp['sum_sentiment'][(df_temp.pos<df_temp.neg)&(df_temp.neg>=df_temp.neut)] = -1\n",
    "\n",
    "    # дивергенция c добавлением \n",
    "                                    df_temp['diver'] = ((df_temp.pos - df_temp.neg)**2 \\\n",
    "                                                              / (df_temp.pos + df_temp.neg)**2)**(1/2)\n",
    "\n",
    "                                    #df_temp.reset_index(level=[f'change_close_{day}', 'date', 'bh'], inplace=True)\n",
    "                                    df_temp['clear_return'] = df_temp.sum_sentiment * df_temp[f'change_close_{day}']\n",
    "                                    df_temp['diver_return'] = df_temp.clear_return * df_temp.diver                \n",
    "\n",
    "                                    res = dict()\n",
    "                                    res['counter'] = [counter]\n",
    "                                    res['ticker'] = [ticker]\n",
    "                                    res['date'] = [day]\n",
    "\n",
    "                                    res['min_df'] = [min_df]\n",
    "\n",
    "                                    res['j_neut'] = [j_neut]\n",
    "                                    res['j_pos'] = [j_pos] \n",
    "                                    res['j_neg'] = [j_neg] \n",
    "\n",
    "                                    res['train_size'] = [y_train.shape[0]]\n",
    "                                    res['test_size'] = [y_pred.shape[0]]\n",
    "\n",
    "                                    res['obs'] = \\\n",
    "                                    [(df_temp.obs[df_temp.clear_return.isna()==False]).sum()]\n",
    "                                    res['bh'] = df_temp.bh.max()\n",
    "                                    res['original_return_isna_all'] = \\\n",
    "                                    [(df_temp[f'change_close_{day}']\\\n",
    "                                      [df_temp[f'change_close_{day}'].isna()==False]).sum()]\n",
    "\n",
    "\n",
    "                                    res['model_return_isna_all'] = \\\n",
    "                                    [(df_temp.clear_return[df_temp.clear_return.isna()==False]).sum()]\n",
    "\n",
    "                                    res['model_return_isna_short'] = \\\n",
    "                                    [(df_temp.clear_return[(df_temp.clear_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=1)]).sum()]\n",
    "\n",
    "                                    res['model_return_isna_long'] = \\\n",
    "                                    [(df_temp.clear_return[(df_temp.clear_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=-1)]).sum()]\n",
    "\n",
    "\n",
    "                                    res['model_diver_return_isna_all'] = \\\n",
    "                                    [(df_temp.diver_return[df_temp.diver_return.isna()==False]).sum()]\n",
    "\n",
    "                                    res['model_diver_return_isna_short'] = \\\n",
    "                                    [(df_temp.diver_return[(df_temp.diver_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=1)]).sum()]\n",
    "\n",
    "                                    res['model_diver_return_isna_long'] = \\\n",
    "                                    [(df_temp.diver_return[(df_temp.diver_return.isna()==False) & \\\n",
    "                                                                 (df_temp.sum_sentiment!=-1)]).sum()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                    res['train_accuracy'] = [metrics.accuracy_score(y_train_pred, y_train)]\n",
    "\n",
    "\n",
    "                                    res['train_precision_macro'] = [metrics.precision_score(y_train_pred, y_train\\\n",
    "                                                                                            , average='macro')]\n",
    "\n",
    "\n",
    "                                    res['train_precision_micro'] = [metrics.precision_score(y_train_pred, y_train\\\n",
    "                                                                                            , average='micro')]\n",
    "\n",
    "\n",
    "                                    res['train_recall_micro'] = [metrics.recall_score(y_train_pred, y_train\\\n",
    "                                                                                      , average='micro')]\n",
    "\n",
    "\n",
    "                                    res['train_recall_macro'] = [metrics.recall_score(y_train_pred, y_train\\\n",
    "                                                                                      , average='macro')]\n",
    "\n",
    "                                    res['train_f1_micro'] = [metrics.f1_score(y_train_pred, y_train\\\n",
    "                                                                              , average='micro')]\n",
    "\n",
    "\n",
    "                                    res['train_f1_macro'] = [metrics.f1_score(y_train_pred, y_train\\\n",
    "                                                                              , average='macro')]\n",
    "\n",
    "\n",
    "\n",
    "                                    x = df_temp[['sum_sentiment', 'close_lag_1']]\n",
    "                                    x.sum_sentiment = x.sum_sentiment.astype(str)\n",
    "\n",
    "                                    x = pd.get_dummies(x, drop_first=True)\n",
    "                                    y = df_temp.close\n",
    "                                    x2 = sm.add_constant(x)\n",
    "                                    lm = sm.OLS(y, x2)\n",
    "                                    res_lm = lm.fit()\n",
    "\n",
    "                                    res['const'] = res_lm.params[0]\n",
    "                                    res[f'{x.columns[0]}_coef'] = res_lm.params[1]\n",
    "                                    res[f'{x.columns[1]}_coef'] = res_lm.params[2]\n",
    "                                    res[f'{x.columns[2]}_coef'] = res_lm.params[3]\n",
    "\n",
    "                                    res['const_p'] = res_lm.pvalues[0]\n",
    "                                    res[f'{x.columns[0]}_p'] = res_lm.pvalues[1]\n",
    "                                    res[f'{x.columns[1]}_p'] = res_lm.pvalues[2]\n",
    "                                    res[f'{x.columns[2]}_p'] = res_lm.pvalues[3]\n",
    "                                    res['r2'] = res_lm.rsquared\n",
    "                                    res['nobs'] = res_lm.nobs\n",
    "                                    counter += 1\n",
    "                                    print(counter, end='\\r')\n",
    "\n",
    "\n",
    "                                    res = pd.DataFrame.from_dict(res)\n",
    "                                    \n",
    "                                    out = out.append(pd.DataFrame(res))\n",
    "                                    if counter % 1000 == 0: \n",
    "                                        out.to_excel(f'/is/res/nb/nb_model_ind_final2.xlsx', index=False)\n",
    "                                    \n",
    "                    except:\n",
    "                        continue\n",
    "    \n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "out.to_excel(f'/is/res/nb/nb_model_ind_final2.xlsx', index=False)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2d395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
